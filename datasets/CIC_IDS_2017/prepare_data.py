#!/usr/bin/env python3
import argparse
import os
import sys
import logging
import time
from pprint import pformat
import pandas as pd
import numpy as np
import re
import zipfile


def read_csv(filename, header_row=0, dtypes=None, columns_to_read=None):
    t0 = time.time()
    logging.info("Reading CSV dataset {}".format(filename))

    if columns_to_read is not None:
        dataset_df = pd.read_csv(filename, header=header_row, dtype=dtypes, usecols=columns_to_read)
    else:
        dataset_df = pd.read_csv(filename, header=header_row, dtype=dtypes)

    logging.info("Reading complete. time_to_read={:.2f} sec".format(time.time() - t0))

    return dataset_df


def write_to_hdf(df, filename, key, compression_level, mode="a", fmt="fixed"):
    logging.info("Writing dataset to HDF5 format. filename={}".format(filename))
    t0 = time.time()

    df.to_hdf(filename, key=key, mode=mode, complevel=compression_level, complib="zlib", format=fmt)

    logging.info("Writing complete. time_to_write={}".format(time.time() - t0))


def load_datasets(files_list, header_row=0, dtypes=None, columns_to_read=None):
    logging.info("Loading datasets in files")

    dfs = []
    for filename in files_list:
        df = read_csv(filename, header_row, dtypes=dtypes, columns_to_read=columns_to_read)
        dfs.append(df)

    all_data = pd.concat(dfs, ignore_index=True)

    all_data.columns = all_data.columns.str.strip()
    all_data.columns = all_data.columns.str.replace(" ", "_")

    logging.info("Loading datasets complete")
    return all_data


def count_labels(label_col):
    label_counts = label_col.value_counts(dropna=False)
    total_count = sum(label_counts)

    logging.info("Total count = {}".format(total_count))

    label_percentages = label_counts / total_count

    return label_counts, label_percentages


def initial_setup(params):
    if not os.path.exists(params["output_dir"]):
        os.makedirs(params["output_dir"])

    # Setup logging
    log_filename = params["output_dir"] + "/" + "run_log.log"

    logging.basicConfig(
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[logging.FileHandler(log_filename, "w+"), logging.StreamHandler()],
        level=logging.INFO
    )
    logging.info("Initialized logging. log_filename = {}".format(log_filename))

    logging.info("Running script with following parameters\n{}".format(pformat(params)))


def split_meta_datasets(all_data, params):
    # ignore labels with small number of datapoint
    df = all_data.groupby("Label").filter(lambda x: len(x) >= params["sample_per_class"]).copy()
    df = df.groupby("Label", group_keys=False).apply(lambda x: x.sample(params["sample_per_class"]))

    labels = df["Label"].unique().copy()
    np.random.shuffle(labels)
    meta_train_labels = labels[:-params["meta_test_class_count"]]
    meta_test_labels = labels[-params["meta_test_class_count"]:]
    meta_train_df = df[df["Label"].isin(meta_train_labels)].copy()
    meta_test_df = df[df["Label"].isin(meta_test_labels)].copy()

    meta_val_df_from_train = meta_train_df.groupby("Label", group_keys=False).apply(lambda x: x.sample(params["validation_sample_per_class"])).copy()
    meta_train_df = meta_train_df.drop(meta_val_df_from_train.index)
    meta_val_df_from_test = meta_test_df.groupby("Label", group_keys=False).apply(lambda x: x.sample(params["validation_sample_per_class"])).copy()
    meta_test_df = meta_test_df.drop(meta_val_df_from_test.index)
    meta_val_df = pd.concat([meta_val_df_from_train, meta_val_df_from_test])

    remaining_df = all_data.drop(meta_train_df.index)  # save for later usages
    remaining_df = remaining_df.drop(meta_test_df.index)  # save for later usages
    remaining_df = remaining_df.drop(meta_val_df.index)  # save for later usages

    x_meta_train = meta_train_df.loc[:, meta_train_df.columns != "Label"]
    y_meta_train = meta_train_df["Label"]

    x_meta_test = meta_test_df.loc[:, meta_test_df.columns != "Label"]
    y_meta_test = meta_test_df["Label"]

    x_meta_val = meta_val_df.loc[:, meta_val_df.columns != "Label"]
    y_meta_val = meta_val_df["Label"]

    logging.info("Train set :\n{}".format(meta_train_df["Label"].value_counts()))
    logging.info("Test set :\n{}".format(meta_test_df["Label"].value_counts()))
    logging.info("Validation set :\n{}".format(meta_val_df["Label"].value_counts()))
    logging.info("Remaining df :\n{}".format(remaining_df["Label"].value_counts()))

    return x_meta_train, y_meta_train, x_meta_test, y_meta_test, x_meta_val, y_meta_val, remaining_df


def pre_process_dataset(params):
    # Load data
    logging.info("Loading datasets")
    data_files_list = [params["ids2017_datasets_dir"] + "/" + filename for filename in params["ids2017_files_list"]]
    all_data = load_datasets(data_files_list, header_row=0)

    # Remove unicode values in class labels
    logging.info("Converting unicode labels to ascii")
    all_data["Label"] = all_data["Label"].apply(lambda x: x.encode("ascii", "ignore").decode("utf-8"))
    all_data["Label"] = all_data["Label"].apply(lambda x: re.sub(" +", " ", x))  # Remove double spaces

    # Following type conversion and casting (both) are necessary to convert the values in cols 14, 15 detected as objects
    # Otherwise, the training algorithm does not work as expected
    logging.info("Converting object type in columns 14, 15 to float64")
    all_data["Flow_Bytes/s"] = all_data["Flow_Bytes/s"].apply(lambda x: np.float64(x))
    all_data["Flow_Packets/s"] = all_data["Flow_Packets/s"].apply(lambda x: np.float64(x))
    all_data["Flow_Bytes/s"] = all_data["Flow_Bytes/s"].astype(np.float64)
    all_data["Flow_Packets/s"] = all_data["Flow_Packets/s"].astype(np.float64)

    logging.info("Removing invalid values (inf, nan)")
    prev_rows = all_data.shape[0]
    all_data.replace([np.inf, -np.inf], np.nan, inplace=True)
    all_data.dropna(inplace=True)  # Some rows (1358) have NaN values in the Flow Bytes/s column. Get rid of them
    logging.info("Removed no. of rows = {}".format(prev_rows - all_data.shape[0]))

    # Check class labels
    label_counts, label_perc = count_labels(all_data["Label"])
    logging.info("Label counts below")
    logging.info("\n{}".format(label_counts))

    logging.info("Label percentages below")
    pd.set_option("display.float_format", "{:.4f}".format)
    logging.info("\n{}".format(label_perc))
    return all_data


def prepare_ids2017_datasets(params):
    all_data = pre_process_dataset(params)

    x_meta_train, y_meta_train, x_meta_test, y_meta_test, x_meta_val, y_meta_val, remaining_df = split_meta_datasets(all_data, params)
    # Save data files in HDF format
    logging.info("Saving prepared datasets (train, val, test) to: {}".format(params["output_dir"]))

    write_to_hdf(x_meta_train, params["output_dir"] + "/" + "x_meta_train.h5", params["hdf_key"], 5)
    write_to_hdf(y_meta_train, params["output_dir"] + "/" + "y_meta_train.h5", params["hdf_key"], 5)

    write_to_hdf(x_meta_test, params["output_dir"] + "/" + "x_meta_test.h5", params["hdf_key"], 5)
    write_to_hdf(y_meta_test, params["output_dir"] + "/" + "y_meta_test.h5", params["hdf_key"], 5)

    write_to_hdf(x_meta_val, params["output_dir"] + "/" + "x_meta_val.h5", params["hdf_key"], 5)
    write_to_hdf(y_meta_val, params["output_dir"] + "/" + "y_meta_val.h5", params["hdf_key"], 5)

    write_to_hdf(remaining_df, params["output_dir"] + "/" + "remaining_df.h5", params["hdf_key"], 5)

    logging.info("Saving complete")

    logging.info("Meta train dataset shape = {}".format(x_meta_train.shape))
    logging.info("Meta test dataset shape = {}".format(x_meta_test.shape))
    logging.info("Meta val dataset shape = {}".format(x_meta_val.shape))
    logging.info("{} Labels used int meta train = \n{}".format(len(y_meta_train.unique()), ", ".join([str(x) for x in y_meta_train.unique()])))
    logging.info("{} Labels used int meta test = \n{}".format(len(y_meta_test.unique()), ", ".join([str(x) for x in y_meta_test.unique()])))
    logging.info("{} Labels used int meta val = \n{}".format(len(y_meta_val.unique()), ", ".join([str(x) for x in y_meta_val.unique()])))


def extract_databases():
    # dataset_zips = ["GeneratedLabelledFlows", "MachineLearningCSV"]
    dataset_zips = ["MachineLearningCSV"]
    for dataset_zip_name in dataset_zips:
        if not os.path.isdir(dataset_zip_name):
            with zipfile.ZipFile(dataset_zip_name + ".zip", "r") as zip_ref:
                zip_ref.extractall(dataset_zip_name)


def parse_configuration():
    parser = argparse.ArgumentParser()
    parser.add_argument("--hdf_key", type=str, default="cic_ids_2017")
    parser.add_argument("--output_dir_prefix", type=str, default="cic_ids_2017_prepared")
    parser.add_argument("--sample_per_class", type=int, default=36)
    parser.add_argument("--meta_test_class_count", type=int, default=4)
    parser.add_argument("--validation_sample_per_class", type=int, default=10)
    parser.add_argument("--ids2017_datasets_dir", type=str, default="MachineLearningCSV/MachineLearningCVE")

    config = parser.parse_args()

    params = {
        "output_dir": "{}_{}-way_test_{}-per_label_{}-per_val".format(config.output_dir_prefix, config.meta_test_class_count, config.sample_per_class, config.validation_sample_per_class),
        "ids2017_files_list": [
            "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv",
            "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv",
            "Friday-WorkingHours-Morning.pcap_ISCX.csv",
            "Monday-WorkingHours.pcap_ISCX.csv",
            "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv",
            "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv",
            "Tuesday-WorkingHours.pcap_ISCX.csv",
            "Wednesday-workingHours.pcap_ISCX.csv"
        ]
    }

    for arg in vars(config):
        params[arg] = getattr(config, arg)

    return params


if __name__ == "__main__":
    parameters = parse_configuration()
    extract_databases()
    initial_setup(parameters)
    prepare_ids2017_datasets(parameters)
    logging.info("Data preparation complete")
